\documentclass[12pt, a4paper]{article}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}

\title{----------------------------------------------------------- \\
        {\bf Programming of Supercomputers WS12/13}\\ 
        ----------------------------------------------------------- \\ 
        Final report}
\author{Marco Seravalli}
\date{January 24th 2013}

\newcommand{\tab}{\hspace{10mm}}
\newcommand{\draft}[1]{\textcolor{NavyBlue}{#1}}
\newcommand{\hint}[1]{\textcolor{OliveGreen}{{\it#1}}}
         
\begin{document}
  \maketitle

\section{Introduction}
The Programming of Supercomputers laboratory is about understanding how already
applications can be modified and optimized in order to be run efficiently and
effectively on massively parallel machines.

The laboratory is divided into two main parts: the first one is about
understanding how a sequential code can be optimized by acting on the input
phase and on the compilation phase.

The second part is instead more concerned about parallelizing a sequential
Computational Fluid Dynamic application. This process is performed throughout 
four steps (milestones).
\begin{itemize}
  \item In the first phase the domain has to be decomposed using different 
        strategies. First of all a straightforward approach is adopted: here 
        contiguous blocks of elements are assigned to a single processor without
        taking the actual position into account. Afterwards also some advanced
        partitioning algorithms provided by the \verb=metis= library are
        introduced namely \verb=metis-dual= and \verb=metis-nodal=.
  \item The second stage consists in building the communication lists that have 
        to be used for the successive computation part. In this phase also other 
        positioning arrays have to be adapted for being able to respect the 
        correctness of the communication.
  \item The third step involves the actual parallelization of the computational
        loop. This requires to understand how the data within the loop should be
        exchanged during the computation.
  \item The aim of the fourth and last part is the optimization of the code in
        order to be able to deliver a better performing application that
        satisfies some predefined standards. The particular objective in this
        case is to reduce the communication overhead for a specific problem 
        under the threshold of 25\%. 
\end{itemize}

\section{Sequential optimization}
The sequential optimization consists in applying different compiler flags to
the code in order to see the effects produced by those in terms of speedup,
cache hit improvements and floating point operations per second (flops).
Then the optimizations are validated by running the application with different
input files.
In following graphs it is possible to see how the various compiler optimization
affected the execution time (Figure \ref{fig:exec_time}) and the L2 cache miss 
rate (Figure \ref{fig:cache_misses}).
\begin{figure}[h]
  \begin{centering}
    \includegraphics[width=1\textwidth]{figures/exec_time.png}
    \par\end{centering}
  \caption{Execution time for each phase. \label{fig:exec_time}}
\end{figure}

\begin{figure}[h]
  \begin{centering}
    \includegraphics[width=1\textwidth]{figures/cache.png}
    \par\end{centering}
  \caption{L2 cache miss rate. \label{fig:cache_misses}}
\end{figure}

Furthermore, also the file reading phase is modified from text to binary. This 
means that the input files are converted from ASCII values to a binary 
representation.
Also this change provides good speed enhancements to the file reading phase. 
The improvement can be quantified in one order of one order of magnitude. In
Figure \ref{fig:io} it can be seen the execution improvement and how the
it is related to the size of the input file.
\begin{figure}[h]
  \begin{centering}
    \includegraphics[width=1\textwidth]{figures/io.png}
    \par\end{centering}
  \caption{
    Execution time of the input phase in relation to the input file size. 
    \label{fig:io}
  }
\end{figure}

\section{Benchmark parallelization}

  \subsection*{Data distribution}
  The data were distributed using different partitioned using different schemes.
  In particular the classical distribution and two different distributions
  offered by the Metis partitioning library.

  For the classical distribution the partitioning was performed by splitting the
  domain composed of $n$ cells equally among the computing units $np$ in a
  sequential way, i.e. the first $ \frac{n}{np} $ cells are assigned to the
  first processor.

  For the two other distributions the whole array containing the cells is passed
  to the library. For more information about how Metis partitions the domain
  please refer to TODO.

  The partitioning was firstly performed on a single processor, eventually in
  order to decrease the communication among the computing units, this step was
  computed in parallel by every single processor.

  The different partitioning strategies are handled by two different functions
  and for these the code duplication is reduced to the minimum the partitioning
  functions only perform the strictly necessary tasks. The strategy was to try
  to shift as mush as possible in the calling function.

  \subsection*{Communication model}
  For the communication model send and receive lists have been used in
  particular it has been necessary to guarantee that the received data was
  actually received at the correct position. In addition to that lists for
  knowing how many elements have to be sent and received from each processor are
  initialized as well.

  Once the communications lists are created the global to local index can be
  created from those. The idea for this index is to first to see what elements
  do belong to a specific processor and assign them an incremental number. Then
  during a successive traversal the ghost cells have incrementally numbered.
  The reason for such numbering is mainly due to the successive phase the
  building of the local to global index. In this index, we want to have the
  inner elements at the beginning of the array and the ghost cells at the end.
  Since the global to local index holds that specific structure it is
  successively possible to initialize local to global (TODO: better formatting)
  in a single traversal as it can be seen in the following algorithm.
  TODO: show algo??

  After having the indexes set it is also possible to modify the communication 
  lists and the lcc array in order to have the local indexes of the elements. 
  This allows to apply only a minimum amount of changes in the original code.
  A further issue needs to be now solved, the external cells, i.e. the cells
  outside the boundaries. The applications treats all these cells in the very
  same way and their coefficients are always $0$. It follows that all the
  information regarding these elements can be stored in a single position. This
  is performed by making the coefficient arrays one element longer and storing a
  this additional information in the last cell. Of course also lcc needs to be
  modified accordingly, such that all the external cells point to the very same
  position in the coefficient arrays.

  \subsection*{MPI implementation}
  For the MPI implementation different aspects should be taken into account:
  \begin{itemize}
    \item Point to point operations: the communication among the different
          processors is handled by point to point operations. In such scenario
          every process sends the information only to another processor at a
          time.
    \item Non blocking communication: if blocking point to point communication 
          is used, deadlocks might occur. The reason is that the some processes
          need to send data before being able to receive information, If it
          happens that all processes communicate will all processes (there is a
          clique), it might occur the case that all processes are ready to
          send, but none is ready to receive causing then a deadlock. This can
          be avoided by using non blocking communication. The sending function
          returns immediately even if the data are not actually send. Also for
          the receiving non blocking communication can be exploited. In this
          case the advantage would be that the incoming data from all processes
          can be received in parallel. Eventually, before performing further
          operations on the received and sent data there is the necessity of
          waiting for the communication to be completed.
    \item Indexed data: for sending data indexed data types are adopted. With
          this data type model MPI sends only the necessary elements from the
          correct position for each array. When receiving the data another
          indexed data type should be used instead because the data has to be
          received in the correct positions of the array. In the current
          implementation, there are created send and receive data types for
          every process with whom there is a communication. The building of
          these indexes is performed by using the send and receive lists
          previously created. 
          TODO: put the algo
    \item Collective operations: some of the variables of the serial
          implementation are computed taking into account all elements of the
          domain. Of course the parallel code needs to take into account this
          computations and one of the most efficient ways to compute these
          values is via collective operations. In particular for the given
          implementation \verb=MPI_Allreduce=  was adopted. This
          function allows to all the processes to share the result once it is
          computed, hence afterwards no further communication is required.
  \end{itemize}

\section{Performance analysis and tuning}
To reduce the communication overhead, all the duplicates from the communication
were deleted so that only the needed information was exchanged.

\section{Overview}

\end{document}

