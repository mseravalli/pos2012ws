\documentclass[12pt, a4paper]{article}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}

\title{----------------------------------------------------------- \\
        {\bf Programming of Supercomputers WS12/13}\\ 
        ----------------------------------------------------------- \\ 
        Final report}
\author{Marco Seravalli}
\date{January 24th 2013}

\newcommand{\tab}{\hspace{10mm}}
\newcommand{\draft}[1]{\textcolor{NavyBlue}{#1}}
\newcommand{\hint}[1]{\textcolor{OliveGreen}{{\it#1}}}
         
\begin{document}
  \maketitle

\section{Introduction}
ADD the first part.

The purpose of the project was to parallelize a sequential CFD application. The
whole process had been performed throughout four steps (milestones).

In the first phase the domain had to be decomposed using different strategies,
namely a classical one, metis-dual and metis-nodal. 
MORE HERE.

The second stage consisted in building the communication lists that had to be
used for the computation part. In this phase also other positioning arrays had
to be adapted for being able to respect the correctness in the communication.

The third step involved the actual parallelization of the computational loop.
The work

The aim of the fourth and last part was the optimization of the code in order to
be able to deliver a better performing application that satisfy some predefined
standards. In particular the objective was to reduce the communication overhead
for a specific problem under the threshold of 25\%. 

\section{Sequential optimization}

\section{Benchmark parallelization}

  \subsection*{Data distribution}
  The data were distributed using different partitioned using different schemes.
  In particular the classical distribution and two different distributions
  offered by the Metis partitioning library.

  For the classical distribution the partitioning was performed by splitting the
  domain composed of $n$ cells equally among the computing units $np$ in a
  sequential way, i.e. the first $ \frac{n}{np} $ cells are assigned to the
  first processor.

  For the two other distributions the whole array containing the cells is passed
  to the library. For more information about how Metis partitions the domain
  please refer to TODO.

  The partitioning was firstly performed on a single processor, eventually in
  order to decrease the communication among the computing units, this step was
  computed in parallel by every single processor.

  The different partitioning strategies are handled by two different functions
  and for these the code duplication is reduced to the minimum the partitioning
  functions only perform the strictly necessary tasks. The strategy was to try
  to shift as mush as possible in the calling function.

  \subsection*{Communication model}
  For the communication model send and receive lists have been used in
  particular it has been necessary to guarantee that the received data was
  actually received at the correct position.

  Once the communications lists were created the global to local index could be
  created from those. The idea for this index was to first to see what elements
  do belong to a specific processor and assign them an incremental number. Then
  during a successive traversal the ghost cells had been incrementally numbered.
  The reason for such numbering is mainly due to the successive phase the
  building of the local to global index. In this index, we want to have the
  inner elements at the beginning of the array and the ghost cells at the end.
  Since the global to local index holds that specific structure it is
  successively possible to initialize local_to_global (TODO: better formatting)
  in a single traversal as it can be seen in the following algorithm.
  TODO: show algo??

  Eventually set the communication lists and lcc to local in order to be able to
  operate with by applying only a minimum amount of changes in the original
  code.

  \subsection*{MPI implementation}
  For the MPI implementation different aspects should be taken into account:
  \begin{itemize}
    \item Point to point operations
    \item Asynchronous communication
    \item Indexed data
    \item Collective operations
  \end{itemize}

\section{Performance analysis and tuning}
To reduce the communication overhead, all the duplicates from the communication
were deleted so that only the needed information was exchanged.

\section{Overview}

\end{document}

