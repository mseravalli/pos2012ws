\documentclass[12pt, a4paper]{article}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}

\title{----------------------------------------------------------- \\
        {\bf Programming of Supercomputers WS12/13}\\ 
        ----------------------------------------------------------- \\ 
        Final report}
\author{Marco Seravalli}
\date{January 24th 2013}

\newcommand{\tab}{\hspace{10mm}}
\newcommand{\draft}[1]{\textcolor{NavyBlue}{#1}}
\newcommand{\hint}[1]{\textcolor{OliveGreen}{{\it#1}}}
         
\begin{document}
  \maketitle

\section{Introduction}
ADD the first part.

The purpose of the project was to parallelize a sequential CFD application. The
whole process had been performed throughout four steps (milestones).

In the first phase the domain had to be decomposed using different strategies,
namely a classical one, metis-dual and metis-nodal. 
MORE HERE.

The second stage consisted in building the communication lists that had to be
used for the computation part. In this phase also other positioning arrays had
to be adapted for being able to respect the correctness in the communication.

The third step involved the actual parallelization of the computational loop.
The work

The aim of the fourth and last part was the optimization of the code in order to
be able to deliver a better performing application that satisfy some predefined
standards. In particular the objective was to reduce the communication overhead
for a specific problem under the threshold of 25\%. 

\section{Sequential optimization}

\section{Benchmark parallelization}

  \subsection*{Data distribution}
  The data were distributed using different partitioned using different schemes.
  In particular the classical distribution and two different distributions
  offered by the Metis partitioning library.

  For the classical distribution the partitioning was performed by splitting the
  domain composed of $n$ cells equally among the computing units $np$ in a
  sequential way, i.e. the first $ \frac{n}{np} $ cells are assigned to the
  first processor.

  For the two other distributions the whole array containing the cells is passed
  to the library. For more information about how Metis partitions the domain
  please refer to TODO.

  The partitioning was firstly performed on a single processor, eventually in
  order to decrease the communication among the computing units, this step was
  computed in parallel by every single processor.

  The different partitioning strategies are handled by two different functions
  and for these the code duplication is reduced to the minimum the partitioning
  functions only perform the strictly necessary tasks. The strategy was to try
  to shift as mush as possible in the calling function.

  \subsection*{Communication model}
  For the communication model send and receive lists have been used in
  particular it has been necessary to guarantee that the received data was
  actually received at the correct position. In addition to that lists for
  knowing how many elements have to be sent and received from each processor are
  initialized as well.

  Once the communications lists are created the global to local index can be
  created from those. The idea for this index is to first to see what elements
  do belong to a specific processor and assign them an incremental number. Then
  during a successive traversal the ghost cells have incrementally numbered.
  The reason for such numbering is mainly due to the successive phase the
  building of the local to global index. In this index, we want to have the
  inner elements at the beginning of the array and the ghost cells at the end.
  Since the global to local index holds that specific structure it is
  successively possible to initialize local to global (TODO: better formatting)
  in a single traversal as it can be seen in the following algorithm.
  TODO: show algo??

  After having the indexes set it is also possible to modify the communication 
  lists and the lcc array in order to have the local indexes of the elements. 
  This allows to apply only a minimum amount of changes in the original code.
  A further issue needs to be now solved, the external cells, i.e. the cells
  outside the boundaries. The applications treats all these cells in the very
  same way and their coefficients are always $0$. It follows that all the
  information regarding these elements can be stored in a single position. This
  is performed by making the coefficient arrays one element longer and storing a
  this additional information in the last cell. Of course also lcc needs to be
  modified accordingly, such that all the external cells point to the very same
  position in the coefficient arrays.

  \subsection*{MPI implementation}
  For the MPI implementation different aspects should be taken into account:
  \begin{itemize}
    \item Point to point operations: the communication among the different
          processors is handled by point to point operations. In such scenario
          every process sends the information only to another processor at a
          time.
    \item Asynchronous communication: if blocking point to point communication 
          is used, deadlocks might occur. The reason is that the some processes
          need to send data before being able to receive information, if it
          happens that all processes communicate will all processes (there is a
          clique), no process is actually 
    \item Indexed data
    \item Collective operations
  \end{itemize}

\section{Performance analysis and tuning}
To reduce the communication overhead, all the duplicates from the communication
were deleted so that only the needed information was exchanged.

\section{Overview}

\end{document}

