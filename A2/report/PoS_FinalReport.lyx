#LyX file created by tex2lyx 2.0.2
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble

\usepackage{a4wide}\usepackage{float}\usepackage{listings}\@ifundefined{definecolor}
 {\usepackage{color}}{}


\title{----------------------------------------------------------- \\
        {\bf Programming of Supercomputers WS 12/13}\\ 
        ----------------------------------------------------------- \\ 
        Final report}
\author{Marco Seravalli}
\date{January 24th 2013}

\newcommand{\tab}{\hspace{10mm}}
\newcommand{\draft}[1]{\textcolor{NavyBlue}{#1}}
\newcommand{\hint}[1]{\textcolor{OliveGreen}{{\it#1}}}
         

\end_preamble
\use_default_options false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\paperfontsize 12
\spacing single
\use_hyperref 1
\pdf_bookmarks 1
\pdf_bookmarksnumbered 0
\pdf_bookmarksopen 0
\pdf_bookmarksopenlevel 1
\pdf_breaklinks 0
\pdf_pdfborder 0
\pdf_colorlinks 0
\pdf_backref section
\pdf_pdfusetitle 1
\papersize a4paper
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 0
\use_mathdots 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section

Introduction
\end_layout

\begin_layout Standard

The Programming of Supercomputers laboratory is about understanding how sequential applications can be modified and optimized in order executed efficiently and effectively on massively parallel machines.
\end_layout

\begin_layout Standard

The laboratory is divided into two main parts: the first one is about understanding how a sequential code can be optimized by acting on the input phase and on the compilation phase.
\end_layout

\begin_layout Standard

The second part is instead more focused on parallelizing a sequential Computational Fluid Dynamic application. This process is performed throughout four steps (the milestones in our case). 
\end_layout

\begin_layout Itemize

In the first phase the domain has to be decomposed, better if using different strategies. First of all a straightforward approach was adopted: in this case contiguous blocks of elements are assigned to a single processor without taking the actual position in space into account. Afterwards also some advanced partitioning algorithms provided by the 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=metis=
\end_layout

\end_inset

 library were introduced namely 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=metis-dual=
\end_layout

\end_inset

 and 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=metis-nodal=
\end_layout

\end_inset

. 
\end_layout

\begin_layout Itemize

The second stage consists in building the communication lists that have to be used for the successive computation part. In this phase also other positioning arrays have to be adapted in order to respect the correctness of the communication. 
\end_layout

\begin_layout Itemize

The third step involves the actual parallelization of the computational loop. This requires to understand how the data within the loop will be exchanged during the computation. 
\end_layout

\begin_layout Itemize

The aim of the fourth and last part is the optimization of the code in order to be able to deliver a better performing application that satisfies some predefined standards. The particular objective in this case is to reduce running time and to have a communication overhead for a specific problem under the threshold of 25%. 
\end_layout

\begin_layout Section

Sequential optimization
\end_layout

\begin_layout Standard

The sequential optimization consisted in applying different compiler flags at compilation time in order to see the effects produced by those in terms of speedup, cache hit improvements and floating point operations per second (flops). Then the optimizations were validated by running the application with different input files. In following graphs it is possible to see how the various compiler optimization affect the execution time (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:exec_time"

\end_inset

) and the L2 cache miss rate (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:cache_misses"

\end_inset

). 
\begin_inset Float figure
placement h
wide false
sideways false
status open


\begin_layout Standard
\align center


\begin_inset Graphics 
	filename figures/exec_time.png
	width 100text%

\end_inset

 
\end_layout

\begin_layout Standard


\begin_inset Caption

\begin_layout Standard

Execution time for each phase. 
\begin_inset CommandInset label
LatexCommand label
name "fig:exec_time"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard


\begin_inset Float figure
placement h
wide false
sideways false
status open


\begin_layout Standard
\align center


\begin_inset Graphics 
	filename figures/cache.png
	width 100text%

\end_inset

 
\end_layout

\begin_layout Standard


\begin_inset Caption

\begin_layout Standard

L2 cache miss rate. 
\begin_inset CommandInset label
LatexCommand label
name "fig:cache_misses"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

Furthermore, also the file reading phase is modified from text to binary. This means that the input files are converted from ASCII values to a binary representation. Also this change provides good speed enhancements in the file reading phase. The improvement can be quantified in about one order of magnitude. In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:io"

\end_inset

 we can see the runtime improvement and how this is related to the size of the input file. 
\begin_inset Float figure
placement h
wide false
sideways false
status open


\begin_layout Standard
\align center


\begin_inset Graphics 
	filename figures/io.png
	width 100text%

\end_inset

 
\end_layout

\begin_layout Standard


\begin_inset Caption

\begin_layout Standard

 Execution time of the input phase in relation to the input file size. 
\begin_inset CommandInset label
LatexCommand label
name "fig:io"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard


\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Section

Benchmark parallelization
\end_layout

\begin_layout Subsection*

Data distribution
\end_layout

\begin_layout Standard

The data are distributed using different partitioning schemes. In particular the classical distribution and two different distributions offered by the 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=Metis=
\end_layout

\end_inset

 partitioning library.
\end_layout

\begin_layout Standard

For the classical distribution the partitioning is performed by splitting the domain composed of 
\begin_inset Formula $n$
\end_inset

 cells equally among the computing units (
\begin_inset Formula $np$
\end_inset

 ) in a sequential way, i.e. the first 
\begin_inset Formula $ \frac{n}{np} $
\end_inset

 cells are assigned to the first process, then the second set of 
\begin_inset Formula $ \frac{n}{np} $
\end_inset

 sequential cells is assigned to the second process and so forth.
\end_layout

\begin_layout Standard

For the two other distributions the whole array containing the cells is passed to the functions provided by the 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=Metis=
\end_layout

\end_inset

 library. For more information about how the distribution is performed by these functions please refer to the 
\begin_inset CommandInset href
LatexCommand href
name "Metis Homepage"
target "http://glaros.dtc.umn.edu/gkhome/metis/metis/overview"

\end_inset

.
\end_layout

\begin_layout Standard

The partitioning was firstly performed on a single processor, eventually in order to decrease the communication among the computing units, this step was computed in parallel by every single processor.
\end_layout

\begin_layout Standard

The different partitioning strategies are handled by two different functions and for these the code duplication is reduced to the minimum: the partitioning functions only perform some strictly necessary tasks. In order to achieve this result as mush code as possible was shifted in the calling function.
\end_layout

\begin_layout Subsection*

Communication model
\end_layout

\begin_layout Standard

After the partition the cells are processed by different computing units and it is then necessary to be able to communicate these values among the different processes. For the communication model send and receive lists have been used. In particular it has been necessary to guarantee that the received data was actually received at the correct position. The communication lists specify which of the internal cells have to be sent and which of the cells have to be received. The received cells are not part of the internal domain assigned to every processor and from now on they will be referred as ghost cells. Additional lists containing the number of elements needed to be sent and received from other processors were also introduced.
\end_layout

\begin_layout Standard

Once the communications lists are created the global to local index can be created from those. The idea for this index is first to see what elements belong to a specific processor and assign them an incremental number. Then during a successive traversal the ghost cells are incrementally numbered. The actual implementation can be seen in the following code.
\end_layout

\begin_layout Standard


\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
begin{verbatim}
\end_layout

\end_inset

 // initialise globallocal int count = 0; for (int i = 0; i < elintglob; ++i) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 if (partelems[i] == myrank) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 (*globallocal)[i] = count; ++count; 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 else 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 (*globallocal)[i] = -1; 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

for (int i = 0; i < size; ++i) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 for (int j = 0; j < recvcount[i]; ++j) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 if ((*globallocal)[recvlist[i][j]] == -1) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 (*globallocal)[recvlist[i][j]] = count; ++count; 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

The reason for such numbering is mainly due to the successive phase: the building of the local to global index. In this index, we want to have the inner elements at the beginning of the array and the ghost cells at the end. Since the global to local index holds that specific structure it is possible to initialize 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=local_global=
\end_layout

\end_inset

 in a single traversal as it can be seen in the following code snippet.
\end_layout

\begin_layout Standard


\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
begin{verbatim}
\end_layout

\end_inset

 // after the initialization of globallocal we build up localglobal for (int i = 0; i < elintglob; ++i) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 if (globallocal[i] != -1) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 (*localglobal)[globallocal[i]] = i; 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

After having the indexes set it is also possible to modify the communication lists and the 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=lcc=
\end_layout

\end_inset

 array in order to have the local indexes of the elements. This allows to apply only a minimum amount of changes in the original code. A further issue that had to be solved, regards the external cells, i.e. the cells outside the boundaries. The applications treats all these cells in the very same way and their coefficients are always 
\begin_inset Formula $0$
\end_inset

. It follows that all the information regarding these elements can be stored in a single position. This is performed by making the coefficient arrays one element longer and storing this additional information in the last cell. Of course also 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=lcc=
\end_layout

\end_inset

 needs to be modified accordingly, such that all the external cells point to the very same position in the coefficient arrays. This change yields to the final form of the 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=local_global_index=
\end_layout

\end_inset

 that can be seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:array"

\end_inset

.
\end_layout

\begin_layout Standard


\begin_inset Float figure
placement h
wide false
sideways false
status open


\begin_layout Standard
\align center


\begin_inset Graphics 
	filename figures/array.png
	width 100text%

\end_inset

 
\end_layout

\begin_layout Standard


\begin_inset Caption

\begin_layout Standard

 How elements are stored within the local to global index. 
\begin_inset CommandInset label
LatexCommand label
name "fig:array"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*

MPI implementation
\end_layout

\begin_layout Standard

For the MPI implementation different aspects should be taken into account: 
\end_layout

\begin_layout Itemize

Point to point operations: the exchange of the cells among the different processors is handled by point to point operations. In such scenario every process sends the information only to another processor at a time. 
\end_layout

\begin_layout Itemize

Non blocking communication: if blocking point to point communication is used, deadlocks might occur. The reason is that the some processes need to send data before being able to receive information. If it happens that all processes need to communicate with all processes, it might occur the case that all processes are ready to send, but none is ready to receive, causing then a deadlock. This can be avoided by using non blocking communication. The sending function returns immediately even if the data are not actually send. Also for the receiving non blocking communication can be exploited. In this case the advantage would be that the incoming data from all processes can be received in the background. Eventually, before performing further operations on the received and sent data there is the necessity of waiting for the communication to be completed. 
\end_layout

\begin_layout Itemize

Indexed data: for sending data indexed data types are adopted. With this data type model MPI sends only the necessary elements from the correct position of each array. When receiving the data another indexed data type should be used instead, because the data has to be received in some other positions of the array. In the current implementation, there are created send and receive data types for every process with whom there is a communication. The building of these indexes is performed by using the send and receive lists previously created. 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
begin{verbatim}
\end_layout

\end_inset

 for (int i = 0; i < size; ++i) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 if (sendcount[i] > 0) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 blocklen = (int*) calloc(sendcount[i], sizeof(int)); for (int j = 0; j < sendcount[i]; ++j) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 blocklen[j] = 1; 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 MPITypeindexed(sendcount[i], blocklen, sendlist[i], MPIDOUBLE, &(sendtypes[i])); MPITypecommit(&(sendtypes[i])); free(blocklen); 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 if (recvcount[i] > 0) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 blocklen = (int*) calloc(recvcount[i], sizeof(int)); for (int j = 0; j < recvcount[i]; ++j) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 blocklen[j] = 1; 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 MPITypeindexed(recvcount[i], blocklen, recvlist[i], MPIDOUBLE, &(recvtypes[i])); MPITypecommit(&(recvtypes[i])); free(blocklen); 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
end{verbatim}
\end_layout

\end_inset

 
\end_layout

\begin_layout Itemize

Collective operations: some of the variables of the serial implementation are computed by using all elements of the domain. Of course the parallel code needs to take into account this computations and one of the most efficient ways to compute these values is via collective operations. In particular for the given implementation 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=MPI_Allreduce=
\end_layout

\end_inset

 was adopted. This function allows to all the processes to share the result once it is computed, hence afterwards no further communication is required. 
\end_layout

\begin_layout Section

Performance analysis and tuning
\end_layout

\begin_layout Standard

For this last phase first of all an optimization aim is set, then the code is analysed using different profiling tools and after the analysis some modifications are performed in order to reach the predefined objective.
\end_layout

\begin_layout Subsection*

Performance aims
\end_layout

\begin_layout Standard

The major objective of the performance is to reduce the total execution time of the application with a particular focus on the communication overhead, which should be under 25% for the whole software in the case of using 4 processes.
\end_layout

\begin_layout Subsection*

Profiling
\end_layout

\begin_layout Standard

The code is analyzed using different tools, namely the ScoreP framework in collaboration with the PAPI library and the Periscope tool. ScoreP in particular was used for instrumenting the code and then manually inspect the outcome through Cube, a visualization tool that allows to scan the different dimensions of the profiling result. Periscope was instead employed to automatically find bottlenecks and hot-spots of the application.
\end_layout

\begin_layout Standard

The outcome of the first analysis is that there is particularly high communication overhead in the initialization part. This is due to the fact that the partitioning is performed on a process and then distributed to the others.
\end_layout

\begin_layout Standard

The computational loop presents some overhead only in the 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=Allreduce=
\end_layout

\end_inset

 MPI function. Small overhead is due to the communication functions 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=send=
\end_layout

\end_inset

 and 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=recv=
\end_layout

\end_inset

 because asynchronous communication is adopted since the first parallelization.
\end_layout

\begin_layout Standard

The finalization phase does not present a great overhead because all the data are communicated only once and the largest amount of time is spent in writing information on the disk.
\end_layout

\begin_layout Subsection*

Optimization
\end_layout

\begin_layout Standard

The communication overhead in the first part could be overcome by computing directly the partitioning on all processes in order to minimize communication. Even though in this case more computations are performed, the overall performance is not affected. 
\begin_inset ERT
status collapsed

\begin_layout Standard

% As it can be seen in the following figure TODO:
\end_layout

\begin_layout Standard


\end_layout

\end_inset


\end_layout

\begin_layout Standard

To reduce the communication overhead, all the duplicates from the communication lists are deleted so that only the needed information was exchanged. Unfortunately, this strategy produces a soar in the initialization phase, but a better performance of the computational loop. 
\begin_inset ERT
status collapsed

\begin_layout Standard

% that does not justify its adoption. The effects produced can be seen in
\end_layout

\begin_layout Standard


\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Standard

% 
\backslash
verb=profiling.ods=.
\end_layout

\begin_layout Standard


\end_layout

\end_inset


\end_layout

\begin_layout Standard

Furthermore in order to be able to overlap communication and computation the sending and the receiving of data was modified after the initial implementation. In the first approach the data was communicated at the beginning of the computational cycle using 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=IRecv=
\end_layout

\end_inset

 and 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=ISend=
\end_layout

\end_inset

 in that order.
\end_layout

\begin_layout Standard


\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
begin{verbatim}
\end_layout

\end_inset

 while ( iter < maxiters ) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 update the old direc1 values receive direc1 values send direc1 values ...
\end_layout

\begin_layout Standard

update resvec ... 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

However in order to be able to perform computations while communicating, calculation and the sending of 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=direc1=
\end_layout

\end_inset

 was moved after the computation of 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=resvec=
\end_layout

\end_inset

. This required to rearrange also the first send and the last receive statement as it can be seen in the following pseudo code.
\end_layout

\begin_layout Standard


\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
begin{verbatim}
\end_layout

\end_inset

 update the old direc1 values send direc1 values
\end_layout

\begin_layout Standard

while ( iter < maxiters ) 
\begin_inset ERT
status collapsed

\begin_layout Standard

{
\end_layout

\end_inset

 receive direc1 values ...
\end_layout

\begin_layout Standard

update resvec update the old direc1 values send direc1 values ... 
\begin_inset ERT
status collapsed

\begin_layout Standard

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

receive direc1 values 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

Unfortunately, the improvements yield by this optimization step are not directly visible. One of the possible causes could be the fact that also the reference code contains asynchronous communication and the optimisations performed to not lead to significant changes in the adopted concepts.
\end_layout

\begin_layout Standard

Moreover, also the bottleneck caused by the 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=Allreduce=
\end_layout

\end_inset

 statements cannot be further optimized. The reason lies on the sequential nature of the problem where the next computations rely on the previous ones. In addition to that also non blocking operations cannot be introduced because there is no possible overlap between the communication and the computation: the calculated data have to be immediately sent to the neighbours to calculate the next coefficient.
\end_layout

\begin_layout Subsection*

Experiment results
\end_layout

\begin_layout Standard

The experiments were run on the interactive partition of the Linux Cluster, the specifics of the machine adopted can be found under 
\begin_inset CommandInset href
LatexCommand href
name "lrz.de"
target "http://www.lrz.de/services/compute/linux-cluster/overview/"

\end_inset

. The application was run using two different set of input namely 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=pent=
\end_layout

\end_inset

 and 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=cojack=
\end_layout

\end_inset

 using different partitioning algorithms (classical and Metis dual). Every configuration had been run on an increasing number of processors, from 1 to 64.
\end_layout

\begin_layout Standard

In figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:cojack_common_1-64"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:cojack_dual_1-64"

\end_inset

: it can be seen the different execution times.
\end_layout

\begin_layout Standard


\begin_inset Float figure
placement h
wide false
sideways false
status open


\begin_layout Standard
\align center


\begin_inset Graphics 
	filename figures/cojack_common_1-64.png
	width 100text%

\end_inset

 
\end_layout

\begin_layout Standard


\begin_inset Caption

\begin_layout Standard

 Times in 
\begin_inset Formula $\mu sec$
\end_inset

 of the single parts of Cojack input run on multiple processes using the common partitioning algorithm. 
\begin_inset CommandInset label
LatexCommand label
name "fig:cojack_common_1-64"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard


\begin_inset Float figure
placement h
wide false
sideways false
status open


\begin_layout Standard
\align center


\begin_inset Graphics 
	filename figures/cojack_dual_1-64.png
	width 100text%

\end_inset

 
\end_layout

\begin_layout Standard


\begin_inset Caption

\begin_layout Standard

 Times in 
\begin_inset Formula $\mu sec$
\end_inset

 of the single parts of Cojack input run on multiple processes using the dual partitioning algorithm. 
\begin_inset CommandInset label
LatexCommand label
name "fig:cojack_dual_1-64"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

The particularly high cost of the initialization is due to the fact that the duplicates are removed send and receive lists, in order to decrease the amount of communication needed. The drawback of this procedure is that the lists have to be sorted, which causes the overhead of the initialization part.
\end_layout

\begin_layout Standard

In the context of this laboratory, the proposed solution would be suboptimal because the initialization cost results to be higher than the actual computation cycle. Anyway, for applications that would involve more time steps, so that the computational loop is more consistent in terms of computing power demands, the proposed approach might result to be a better solution. Unfortunately, these tests and a more careful and deep comparison with a code containing duplicates in the lists could not be performed due to the unavailability of the batch and interactive servers.
\end_layout

\begin_layout Standard

In any case, the speedup obtained for the computational cycle presents a good scaling, in particular for small number of processors as it can be seen in figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:scaling_cojack_common_1-64"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:scaling_cojack_dual_1-64"

\end_inset

.
\end_layout

\begin_layout Standard


\begin_inset Float figure
placement h
wide false
sideways false
status open


\begin_layout Standard
\align center


\begin_inset Graphics 
	filename figures/scaling_cojack_common_1-64.png
	width 100text%

\end_inset

 
\end_layout

\begin_layout Standard


\begin_inset Caption

\begin_layout Standard

 Scaling for computational loop for Cojack input run on multiple processes using the common partitioning algorithm. 
\begin_inset CommandInset label
LatexCommand label
name "fig:scaling_cojack_common_1-64"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard


\begin_inset Float figure
placement h
wide false
sideways false
status open


\begin_layout Standard
\align center


\begin_inset Graphics 
	filename figures/scaling_cojack_dual_1-64.png
	width 100text%

\end_inset

 
\end_layout

\begin_layout Standard


\begin_inset Caption

\begin_layout Standard

 Scaling for computational loop for Cojack input run on multiple processes using the dual partitioning algorithm. 
\begin_inset CommandInset label
LatexCommand label
name "fig:scaling_cojack_dual_1-64"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

For high number of processors the application does not benefit from the parallelization any more, because of the increased communication overhead.
\end_layout

\begin_layout Standard

The results for the 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=pent=
\end_layout

\end_inset

 geometry resemble the ones obtained for 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=cojack=
\end_layout

\end_inset

. This data including additional charts regarding all the configurations can be found in the spreadsheet 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=profiling.ods=
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard

Eventually also the communication objective has been fulfilled, both in 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=pent=
\end_layout

\end_inset

 and in 
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
verb=cojack=
\end_layout

\end_inset

 the percentage of time spent in communication is less than 20%, when 4 processes are used. 
\begin_inset ERT
status collapsed

\begin_layout Standard

% TODO: insert graph of communication
\end_layout

\begin_layout Standard


\end_layout

\end_inset


\end_layout

\begin_layout Section

Overview
\end_layout

\begin_layout Standard

The lab course proposed an opportunity to approach the parallelization of an existing code. During the different phases several unexpected challenges had to be solved. In particular, a careful planning of the data structures and the communication had been necessary to be able to reach a working solution.
\end_layout

\end_body
\end_document
